<h1 align = "center"> MODELO(S) REGRESI√ìN LINEAL SIMPLE Y M√öLTIPLE </h1> 

<p align = "center">
  <img src = "media/image.png"/>
</p>
<p align = "center">
  <a href = "https://github.com/youssef-nabaha">
    <img src = "https://img.shields.io/badge/-youssef--nabaha-181717?style=for-the-badge&logo=github&logoColor=white" alt =  "GitHub: youssef-nabaha" />
  </a>
</p>

## √çNDICE

1. **Conceptos Fundamentales:**
    - ¬øQu√© es un modelo en IA?
    - Origen de la regresi√≥n lineal
2. **Librer√≠as Python:**
    - Scikit-learn
    - Statsmodels
    - TensorFlow
    - PyTorch
3. **Regresi√≥n Lineal Simple:**
    - Definici√≥n y ecuaci√≥n
    - M√©tricas (MSE, R¬≤)
    - Ejemplos con c√≥digo
4. **Regresi√≥n Lineal M√∫ltiple:**
    - Definici√≥n y ecuaci√≥n
    - Interpretaci√≥n de coeficientes
    - Ejemplos con c√≥digo

5. **Referencias y Recursos:**

## 1. CONCEPTOS FUNDAMENTALES
### 1.1. ¬øQu√© es un Modelo en Inteligencia Artificial?

Un **modelo** en IA es simplemente una representaci√≥n matem√°tica que aprende patrones de los datos para realizar predicciones o tomar decisiones.

- **Definici√≥n formal**: 
> Un **modelo** es una funci√≥n matem√°tica **M** que mapea un conjunto de entradas **X** a un conjunto de salidas **Y** `M: X -> Y`.

- **Componentes de un modelo:**

  - **Par√°metros:** valores que el modelo aprende (pesos, coeficientes)
  - **Hiperpar√°metros:** valores que nosotros definimos (tasa de aprendizaje, etc.)
  - **Funci√≥n:** ecuaci√≥n matem√°tica que relaciona entrada y salida
  - **Algoritmo de entrenamiento:** m√©todo para ajustar los par√°metros
  
- **Ejemplo concreto:**

Imagina que queremos predecir el **precio** de una casa. Nuestro modelo ser√≠a:
```
Entrada (X): √°rea de la casa (m¬≤)
Salida (Y): precio (euros)
Modelo: Y = a¬∑X + b
Par√°metros: a = 1500, b = 50000
```

Este modelo "aprendi√≥" que cada `m¬≤` vale **1500‚Ç¨** y que hay un precio base de **50000‚Ç¨**.

### 1.2. Origen de la Regresi√≥n Lineal
La **regresi√≥n lineal** fue desarrollada por **Carl Friedrich Gauss** a principios del siglo XIX (1809) para predecir la √≥rbita de asteroides. El m√©todo se populariz√≥ cuando **Adrien-Marie Legendre** public√≥ el m√©todo de los m√≠nimos cuadrados en 1805.

- **¬øPor qu√© se llama "regresi√≥n"?**

El t√©rmino fue acu√±ado por **Francis Galton** en 1886 al estudiar la herencia de la altura. Observ√≥ que los hijos de padres muy altos tend√≠an a ser m√°s bajos que sus padres (regresaban hacia la media), fen√≥meno que llam√≥ **regresi√≥n hacia la mediocridad**.

## 2. LIBRER√çAS PYTHON PARA REGRESI√ìN

### 2.1. [Scikit-learn](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://scikit-learn.org/&ved=2ahUKEwiTl4n7r6-QAxX_gf0HHTQuLH0QFnoECAwQAQ&usg=AOvVaw3pidYsGhglQXGDh_4GMetL)

> **Descripci√≥n:** Librer√≠a de machine learning m√°s popular de Python

- **Instalaci√≥n:**

    `pip install scikit-learn`
- **Ventajas:**
    - Muy f√°cil de usar
    - API consistente
    - Excelente documentaci√≥n
    - R√°pida y eficiente
    - Integraci√≥n perfecta con [pandas](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://pandas.pydata.org/&ved=2ahUKEwiv1IXUsa-QAxW4VqQEHTE8BEMQFnoECAwQAQ&usg=AOvVaw3cD5ulu4AnZcNusojIyttY)
- **Desventajas:**
    - No incluye inferencia estad√≠stica detallada
    - No muestra p-valores autom√°ticamente
- **M√©todos obligatorios:**

    - `fit(X, y)`: entrenar el modelo
    - `predict(X)`: hacer predicciones
    - `score(X, y)`: calcular R¬≤
- **Cu√°ndo usar:**

    - Proyectos de machine learning
    - Cuando necesitas rapidez
    - Producci√≥n y aplicaciones reales
    - Integraci√≥n con pipelines

## 2.2. [Statsmodels](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.statsmodels.org/&ved=2ahUKEwiSy-v3sa-QAxXYTqQEHbGaA20QFnoECBkQAQ&usg=AOvVaw3XgChxrWXjI2WI6XFViKwh)

> **Descripci√≥n:** Librer√≠a enfocada en estad√≠stica e inferencia

- **Instalaci√≥n:**

    `pip install statsmodels`

- **Ventajas:**

    - An√°lisis estad√≠stico completo
    - P-valores e intervalos de confianza
    - Resumen detallado autom√°tico
    - Pruebas de hip√≥tesis
    
 - **Desventajas:**

    - API menos intuitiva
    - M√°s lenta que scikit-learn
    - Requiere a√±adir constante manualmente
    
- **M√©todos obligatorios:**

    - `add_constant(X)`: a√±adir intercepto
    - `OLS(y, X)`: crear modelo
    - `fit()`: entrenar
    - `summary()`: resumen estad√≠stico

- **Cu√°ndo usar:**

    - Investigaci√≥n acad√©mica
    - Necesitas p-valores
    - An√°lisis estad√≠stico profundo
    - Reportes cient√≠ficos


## 2.3. [TensorFlow](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.tensorflow.org/&ved=2ahUKEwiAp_Kv36-QAxUdVKQEHW7FHOYQFnoECAwQAQ&usg=AOvVaw0TGZBeXHx2CVPI2FiDZclR)

> **Descripci√≥n:** Framework de deep learning de Google.

- **Instalaci√≥n:**

    `pip install tensorflow`

- **Ventajas:**

    - Muy flexible
    - GPU acceleration
    - Escalable a grandes datasets
    - Integraci√≥n con redes neuronales

- **Desventajas:**

    - Excesivamente complejo para regresi√≥n simple
    - Curva de aprendizaje pronunciada
    - M√°s lento para modelos simples

- **M√©todos obligatorios:**

    - `Sequential()`: crear modelo
    - `Dense(units)`: capa densa
    - `compile()`: configurar
    - `fit(X, y)`: entrenar

- **Cu√°ndo usar:**

    - Modelos complejos no lineales
    - Necesitas redes neuronales
    - Datasets masivos
    - NO recomendado para regresi√≥n lineal simple

## 2.4. [PyTorch](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://pytorch.org/&ved=2ahUKEwi1t-X-4K-QAxXwNfsDHbxJOaoQFnoECA0QAQ&usg=AOvVaw2mABY6VbqZdRJYnleMzDSb)

> **Descripci√≥n:** Framework de deep learning de Facebook.

- **Instalaci√≥n:**
    Para CPU (default):
    
    `pip install torch torchvision torchaudio`

    
- **Ventajas:**

    - Muy flexible
    - Debugging m√°s f√°cil que TensorFlow
    - Sintaxis pyth√≥nica
    - GPU acceleration

- **Desventajas:**

    - Demasiado complejo para regresi√≥n simple
    - Verboso para modelos b√°sicos
    - M√°s lento que scikit-learn

- **M√©todos obligatorios:**

    - `nn.Linear()`: capa lineal
    - `forward()`: propagaci√≥n
    - `backward()`: gradientes
    - `optim.SGD()`: optimizador

- **Cu√°ndo usar:**

    - Investigaci√≥n en deep learning
    - Modelos personalizados complejos
    - NO recomendado para regresi√≥n lineal simple

## 3. REGRESI√ìN LINEAL SIMPLE

### 3.1. Definici√≥n y ecuaci√≥n

La **regresi√≥n lineal simple** modela la relaci√≥n entre una variable independiente **X** y una variable dependiente **Y** mediante una l√≠nea recta.

- **Ecuaci√≥n:**

 $$
 y = a \cdot x + b
 $$
 
   **Donde:**
   - $y$ = variable **dependiente** (lo que queremos predecir)
   - $x$ = variable **independiente** (lo que usamos para predecir)
   - $a$ = **pendiente** (cu√°nto cambia $y$ cuando $x$ aumenta en 1)
   - $b$ = **intercepto** (valor de $y$ cuando $x$ = 0)


### 3.2. M√©tricas de Evaluaci√≥n

#### MSE (Mean Squared Error - Error Cuadr√°tico Medio)

- **Definici√≥n:**

El **MSE** es el promedio de los errores al cuadrado entre los valores reales y las predicciones.

- **F√≥rmula:**

$$
\mathrm{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_{\text{real},i} - y_{\text{predicho},i})^{2}
$$

**Donde:**
- $n$ = n√∫mero de observaciones

- **Interpretaci√≥n:**

  - MSE = 0: predicciones perfectas
  - MSE peque√±o: buen modelo
  - MSE grande: mal modelo
  - Penaliza mucho los errores grandes (por el cuadrado)

 - **Ejemplo:**
```
Valores reales:    [100, 150, 200]
Predicciones:      [110, 145, 195]
Errores:           [ 10,   5,   5]
Errores¬≤:          [100,  25,  25]

MSE = (100 + 25 + 25) / 3 = 50
```
#### R¬≤ (Coeficiente de Determinaci√≥n)

- **Definici√≥n:**

**R¬≤** mide qu√© proporci√≥n de la varianza de la variable dependiente es explicada por el modelo.

$$
\begin{aligned}
R^{2} &= 1 - \frac{SS_{\text{residual}}}{SS_{\text{total}}} \end{aligned}
$$
**Donde:**
$$
\begin{aligned}
SS_{\text{residual}} &= \sum_{i=1}^{n} (y_{\text{real},i} - y_{\text{predicho},i})^{2} 
\end{aligned}
$$

$$
\begin{aligned}
SS_{\text{total}} &= \sum_{i=1}^{n} (y_{\text{real},i} - \bar{y}_{\text{real}})^{2}
\end{aligned}
$$

 - **Interpretaci√≥n:**

    - R¬≤ = 1.0: modelo perfecto (explica 100% de la varianza)
    - R¬≤ = 0.8: modelo bueno (explica 80% de la varianza)
    - R¬≤ = 0.5: modelo regular
    - R¬≤ = 0.0: modelo no explica nada
    - R¬≤ < 0.0: modelo peor que predecir la media
    
- **Ejemplo:**
```
R¬≤ = 0.85 significa: 
El modelo explica el 85% de la variaci√≥n en los precios. 
El 15% restante se debe a otros factores no incluidos.
```

### 3.3. Ejemplos con c√≥digo
-  **Ejemplo con Sickit-learn:**
> En este ejemplo usaremos como dataset **California Housing**, que contiene informaci√≥n sobre viviendas en California. Usaremos la variable `median_income` (ingreso medio de la zona) para predecir `median_house_value` (valor medio de las casas) mediante **regresi√≥n lineal simple**
```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Cargar datos
df = pd.read_csv("housing.csv")

# Eliminar valores nulos
df = df.dropna()

# Seleccionar variables
X = df[['median_income']]
y = df['median_house_value']

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size = 0.2, random_state = 42
)

# Crear y entrenar modelo
model = LinearRegression()
model.fit(X_train, y_train)

# Obtener par√°metros
a = model.coef_[0]
b = model.intercept_

print(f"Ecuaci√≥n: y = {a:.4f}¬∑median_income + {b:.4f}")

# Predicciones
y_pred = model.predict(X_test)

# M√©tricas
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.4f}")
print(f"R¬≤: {r2:.4f}")

# Gr√°fico
plt.scatter(X_test, y_test, color = "blue", alpha = 0.5, label = "Datos reales")

plt.plot(X_test, y_pred, color = "red", linewidth = 2, label = "Linea de regresi√≥n")

plt.xlabel("Ingreso Medio (median_income)")
plt.ylabel("Valor Medio de la Casa (median_house_value)")
plt.title("Regresi√≥n Lineal Simple mediante Sickit-learn")
plt.legend()
plt.show()
```

> **Nota:** 
**random_state** controla la aleatoriedad -> garantiza que si ejecutas el c√≥digo otra vez, obtengas siempre la misma divisi√≥n entre entrenamiento y prueba
**¬øPor qu√© el n√∫mero 42?**
Es simplemente una broma entre programadores. Proviene del libro *The Hitchhiker‚Äôs Guide to the Galaxy*, donde 42 es ‚Äúla respuesta a la vida, el universo y todo lo dem√°s‚Äù. As√≠ que muchos usan 42 como n√∫mero aleatorio ‚Äúcl√°sico‚Äù
### **Salida:**

<p align = "center">
  <img src = "media/simple-sklearn.png"/>
</p>


```
Ecuaci√≥n: y = 41751.9580¬∑median_income + 45035.2282
MSE: 7221011204.2350
R¬≤: 0.4720
```
### **Interpretaci√≥n:**

* **Ecuaci√≥n:** `y = 41751.96¬∑median_income + 45035.23`
  - Por cada unidad que aumenta el ingreso medio (`median_income`), el valor de la casa aumenta aproximadamente **$41,752**.
  - El intercepto de **$45,035** representa el valor base de una casa cuando el ingreso medio es 0 (aunque en la pr√°ctica este valor no tiene mucho sentido real).

* **MSE = 7,221,011,204.24** -> El error cuadr√°tico medio es muy alto debido a que los valores de las casas est√°n en d√≥lares (valores grandes). En promedio, las predicciones se desv√≠an significativamente del valor real. Para interpretar mejor este valor, es √∫til calcular el **RMSE** (raiz cuadrada del MSE):
  - RMSE = ‚àö7,221,011,204.24 ‚âà **$84,977**
  - Esto significa que, en promedio, las predicciones se alejan aproximadamente **$85,000** del valor real de la casa.

* **R¬≤ = 0.4720** -> El modelo explica el **47.2%** de la variabilidad en los valores de las casas. Esto significa que `median_income` por s√≠ sola es un predictor **moderadamente bueno** para estimar el valor de una casa. Sin embargo, queda un **52.8%** de la variabilidad que no est√° explicada por esta √∫nica variable, lo cual sugiere que **otros factores** (como ubicaci√≥n, tama√±o, antig√ºedad, etc.) tambi√©n influyen significativamente en el precio de las viviendas.

**Conclusi√≥n:** Aunque el ingreso medio tiene una relaci√≥n importante con el valor de las casas (R¬≤ del 47%), claramente **no es suficiente** para hacer predicciones precisas. Para mejorar el modelo, deber√≠amos usar **regresi√≥n lineal m√∫ltiple** incorporando m√°s variables como `housing_median_age`, `total_rooms`, `latitude`, `longitude`, etc.

- **Ejemplo con Statsmodels:**

> Usaremos como ejemplo el mismo dataset **California Housing**

```python
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Cargar datos
df = pd.read_csv('housing.csv')

# Eliminar valores nulos
df = df.dropna()

# Seleccionar la variable independiente
X = df[["median_income"]]

# Sleccionar la variable dependiente
y = df["median_house_value"]

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size = 0.2, random_state = 42
)

# A√±adir constante (intercepto) es obligatorio en statsmodels
X_train_const = sm.add_constant(X_train)
X_test_const = sm.add_constant(X_test)

# Crear modelo
model = sm.OLS(y_train, X_train_const)

# Entrenar modelo
results = model.fit()

# Mostrar resumen estadistico completo
print(results.summary())

# Obtener par√°metros
b = results.params[0]  # Intercepto
a = results.params[1]  # Pendiente

print(f"\nEcuaci√≥n: y = {a:.4f}¬∑median_income + {b:.4f}")

# Predicciones
y_pred = results.predict(X_test_const)

# M√©tricas
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.4f}")
print(f"R¬≤: {r2:.4f}")

# Gr√°fico
plt.scatter(X_test, y_test, color = "blue",alpha = 0.5, label = "Datos reales")
plt.plot(X_test, y_pred, color = "red", linewidth = 2, label = "L√≠nea de regresi√≥n")
plt.xlabel("Ingreso Medio (median_income)")
plt.ylabel("Valor Medio de la Casa (median_house_value)")
plt.title("Regresi√≥n Lineal Simple mediante Statsmodels")
plt.legend()
plt.show()
```
#### **Salida:**

<p align = "center">
  <img src = "media/simple-statsmodels.png"/>
</p>

```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:     median_house_value   R-squared:                       0.474
Model:                            OLS   Adj. R-squared:                  0.474
Method:                 Least Squares   F-statistic:                 1.475e+04
Date:                Tue, 21 Oct 2025   Prob (F-statistic):               0.00
Time:                        02:57:05   Log-Likelihood:            -2.0842e+05
No. Observations:               16346   AIC:                         4.168e+05
Df Residuals:                   16344   BIC:                         4.169e+05
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P>|t|      [0.025      0.975]
---------------------------------------------------------------------------------
const          4.504e+04   1482.200     30.384      0.000    4.21e+04    4.79e+04
median_income  4.175e+04    343.827    121.433      0.000    4.11e+04    4.24e+04
==============================================================================
Omnibus:                     3358.375   Durbin-Watson:                   1.979
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             7418.623
Skew:                           1.184   Prob(JB):                         0.00
Kurtosis:                       5.299   Cond. No.                         10.2
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

Ecuaci√≥n: y = 41751.9580¬∑median_income + 45035.2282
MSE: 7221011204.2350
R¬≤: 0.4720

```

## 4. REGRESI√ìN LINEAL M√öLTIPLE

### 4.1. Definici√≥n y Ecuaci√≥n

La **regresi√≥n lineal m√∫ltiple** es un m√©todo estad√≠stico que modela la relaci√≥n entre dos o m√°s variables **independientes** (predictoras) y una variable **dependiente** (respuesta) mediante una ecuaci√≥n lineal. A diferencia de la **regresi√≥n simple** que usa solo una variable predictora, la regresi√≥n m√∫ltiple permite incorporar varios factores simult√°neamente para mejorar la capacidad predictiva del modelo.

> **Concepto clave:** En el mundo real, los fen√≥menos raramente dependen de un solo factor. 

> **Por ejemplo:**
- El precio de una casa no solo depende del ingreso medio de la zona, sino tambi√©n de su tama√±o, ubicaci√≥n, antig√ºedad, n√∫mero de habitaciones, etc.
- El rendimiento acad√©mico no solo depende de las horas de estudio, sino tambi√©n de la asistencia a clase, conocimientos previos, motivaci√≥n, etc.

La **regresi√≥n m√∫ltiple** nos permite capturar estas relaciones complejas de forma m√°s precisa.

- **Ecuaci√≥n:**

$$
y = a_1 \cdot x_1 + a_2 \cdot x_2 + \dots + a_n \cdot x_n + b
$$

*Donde*:

- $y$: Variable **dependiente** (lo que queremos predecir)
- $x‚ÇÅ, x‚ÇÇ, ..., x‚Çô:$ Variables **independientes** (predictoras o features)
- $a‚ÇÅ, a‚ÇÇ, ..., a‚Çô:$ **Coeficientes** o **pesos** de cada variable independiente
- $b$: T√©rmino **independiente** o **intercepto**
- $n$: **N√∫mero** de variables independientes

- **C√°lculo de ejemplo:**
```
Para una casa con:
- Ingreso medio: 5.0
- Antig√ºedad: 15 a√±os
- Total habitaciones: 2000
- Poblaci√≥n: 1500
```
```
Valor_casa = 41000√ó5.0 + (-800)√ó15 + 15√ó2000 + 20√ó1500 + 50000
          = 205000 + (-12000) + 30000 + 30000 + 50000
          = 303,000$
```

## 4.2. Interpretaci√≥n de Coeficientes

### Concepto Fundamental: [Ceteris Paribus](https://es.wikipedia.org/wiki/Ceteris_paribus)

**Principio clave:** Cada coeficiente $a·µ¢$ representa el cambio en $y$ cuando $x·µ¢$ aumenta en 1 unidad, **manteniendo constantes todas las dem√°s variables**.

En lat√≠n: **"ceteris paribus"** = "todo lo dem√°s permanece igual"

### Importancia de `Ceteris Paribus`

- **¬øPor qu√© es importante este concepto?**

Sin mantener otras variables constantes, no podemos aislar el efecto real de una variable:

**Ejemplo SIN control:**
- Casa A: ingreso = 5.0, antig√ºedad = 10 -> $320,000
- Casa B: ingreso = 6.0, antig√ºedad = 30 -> $310,000

**Conclusi√≥n err√≥nea:** Mayor ingreso lleva a menor valor

**CON control (ceteris paribus):**
- El efecto del ingreso es +$41,000
- El efecto de la antig√ºedad es -$800 por a√±o
- Casa B tiene m√°s ingreso (+$41,000) pero tambi√©n m√°s antig√ºedad (-$16,000)
- Resultado neto: $310,000 

## 4.3. Ejemplos con C√≥digo
- **Ejemplo con Scikit-learn:**

> En este ejemplo usaremos m√∫ltiples variables (`median_income`, `housing_median_age`, `total_rooms`, `population`) para predecir `median_house_value` (valor medio de las casas) mediante **regresi√≥n lineal m√∫ltiple**

```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Cargar datos
df = pd.read_csv('housing.csv')

# Eliminar valores nulos
df = df.dropna()

# Seleccionar las variables independientes
X = df[['median_income', 'housing_median_age', 'total_rooms', 'population']]

# Seleccionar la variable dependiente
y = df['median_house_value']

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size = 0.2, random_state = 42
)

# Crear modelo
model = LinearRegression()

# Entrenar modelo
model.fit(X_train, y_train)

# Obtener par√°metros
coeficientes = model.coef_
b = model.intercept_

# Mostrar ecuaci√≥n
print("Ecuaci√≥n del modelo:\n")
print(
    f"y = {coeficientes[0]:.4f}¬∑median_income + " 
    f"{coeficientes[1]:.4f}¬∑housing_median_age + " 
    f"{coeficientes[2]:.4f}¬∑total_rooms + " 
    f"{coeficientes[3]:.4f}¬∑population + {b:.4f}"
    )


# Predicciones
y_pred = model.predict(X_test)

# M√©tricas
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mse)

print(f"\nMSE: {mse:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"R¬≤: {r2:.4f}")

# Gr√°fico: Valores reales vs predicciones
plt.figure(figsize = (10, 6))
plt.scatter(y_test, y_pred, color = "blue", alpha = 0.5, s = 10)
plt.plot(
    [y_test.min(), y_test.max()], 
    [y_test.min(), y_test.max()], 
    "r--", lw = 2, 
    label = "Predicci√≥n perfecta"
)
plt.xlabel("Valor Real (median_house_value)")
plt.ylabel("Valor Predicho")
plt.title(f"Regresi√≥n Lineal M√∫ltiple (Real vs Predicho) mediante Sickit-learn")
plt.legend()
plt.grid(True, alpha = 0.3)
plt.show()
```

### **salida**:

<p align = "center">
  <img src = "media/multiple-sklearn.png"/>
</p>

```
Ecuaci√≥n del modelo:

y = 41276.4598¬∑median_income + 1966.9217¬∑housing_median_age + 9.1386¬∑total_rooms + -11.3958¬∑population + -17219.7617

MSE: 6602599478.6543
RMSE: 81256.3812
R¬≤: 0.5172
```
### **Interpretaci√≥n:**

* **Ecuaci√≥n:** 
  - Por cada unidad de `median_income`, el valor aumenta **$41,276** (factor m√°s influyente)
  - Por cada a√±o de `housing_median_age`, el valor aumenta **$1,967** (vecindarios antiguos en California suelen ser m√°s valiosos)
  - Por cada habitaci√≥n en `total_rooms`, el valor aumenta **$9.14** (efecto peque√±o pero positivo)
  - Por cada persona en `population`, el valor **disminuye $11.40** (zonas menos densas son m√°s caras)
  - Intercepto de **-$17,220** (sin interpretaci√≥n pr√°ctica real)

* **MSE = 6,602,599,478:** Error cuadr√°tico medio. Dif√≠cil de interpretar por estar en "d√≥lares al cuadrado".

* **RMSE = $81,256:** En promedio, las predicciones se desv√≠an **$81,256** del valor real. Mejor√≥ respecto a la regresi√≥n simple (de ~$85,000 a ~$81,256), una reducci√≥n de **$3,744**.

* **R¬≤ = 0.5172 (51.72%):** El modelo explica el **51.72%** de la variabilidad en los precios. Mejor√≥ del **47.2%** (regresi√≥n simple) al **51.7%** (regresi√≥n m√∫ltiple), un aumento de **+4.5 puntos porcentuales**. A√∫n queda un **48.3%** sin explicar (ubicaci√≥n exacta, caracter√≠sticas espec√≠ficas de la casa, etc.).

* **Conclusi√≥n:** La regresi√≥n m√∫ltiple proporciona predicciones **moderadamente mejores** que la regresi√≥n simple. Al incorporar m√°s variables relevantes (edad, habitaciones, poblaci√≥n), capturamos mejor los factores que determinan el valor de las casas, aunque el modelo a√∫n tiene limitaciones importantes.

- **Ejemplo con Statsmodels:**

```python
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import warnings 

# Cargar datos
df = pd.read_csv('housing.csv')

# Eliminar valores nulos
df = df.dropna()

# Seleccionar las variables independientes
X = df[['median_income', 'housing_median_age', 'total_rooms', 'population']]

# Seleccionar la variable dependiente
y = df['median_house_value']

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size = 0.2, random_state = 42
)

# A√±adir constante (intercepto) obligatorio en statsmodels
X_train_const = sm.add_constant(X_train)
X_test_const = sm.add_constant(X_test)

# Crear modelo
model = sm.OLS(y_train, X_train_const)

# Entrenar modelo
results = model.fit()

# Mostrar resumen estad√≠stico completo
print(results.summary())

# Obtener par√°metros
parametros = results.params
b = parametros.iloc[0]  # Intercepto
coeficientes = parametros.iloc[1:]  # Coeficientes de las variables


# Mostrar ecuaci√≥n
warnings.filterwarnings("ignore") # Ocultar los Warings
print("\n" + "="*70)
print("Ecuaci√≥n del modelo:")
print("="*70 + "\n")
ecuacion = f"y = {coeficientes[0]:.4f}¬∑median_income"
ecuacion += f" + {coeficientes[1]:.4f}¬∑housing_median_age"
ecuacion += f" + {coeficientes[2]:.4f}¬∑total_rooms"
ecuacion += f" + {coeficientes[3]:.4f}¬∑population"
ecuacion += f" + {b:.4f}"
print(ecuacion)


# Predicciones
y_pred = results.predict(X_test_const)

# M√©tricas
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mse)

print("\n" + "="*70)
print("Metricas:")
print("="*70)
print(f"MSE: {mse:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"R¬≤: {r2:.4f}")
print(f"R¬≤ ajustado: {results.rsquared_adj:.4f}")


# Gr√°fico: Valores reales vs predicciones
plt.figure(figsize = (10, 6))
plt.scatter(y_test, y_pred, color = "blue", alpha = 0.5, s = 10)
plt.plot(
    [y_test.min(), y_test.max()], 
    [y_test.min(), y_test.max()], 
    "r--", lw = 2, 
    label = "Predicci√≥n perfecta"
)
plt.xlabel("Valor Real (median_house_value)")
plt.ylabel("Valor Predicho")
plt.title(f"Regresi√≥n Lineal M√∫ltiple mediante Statsmodels")
plt.legend()
plt.grid(True, alpha = 0.3)
plt.show()
```

### **Salida:**

<p align = "center">
  <img src = "media/multiple-statsmodels.png"/>
</p>

```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:     median_house_value   R-squared:                       0.517
Model:                            OLS   Adj. R-squared:                  0.517
Method:                 Least Squares   F-statistic:                     4377.
Date:                Tue, 21 Oct 2025   Prob (F-statistic):               0.00
Time:                        02:44:45   Log-Likelihood:            -2.0772e+05
No. Observations:               16346   AIC:                         4.155e+05
Df Residuals:                   16341   BIC:                         4.155e+05
Df Model:                           4                                         
Covariance Type:            nonrobust                                         
======================================================================================
                         coef    std err          t      P>|t|      [0.025      0.975]
--------------------------------------------------------------------------------------
const              -1.722e+04   2496.260     -6.898      0.000   -2.21e+04   -1.23e+04
median_income       4.128e+04    355.679    116.050      0.000    4.06e+04     4.2e+04
housing_median_age  1966.9217     53.369     36.855      0.000    1862.313    2071.530
total_rooms            9.1386      0.609     15.011      0.000       7.945      10.332
population           -11.3958      1.116    -10.211      0.000     -13.583      -9.208
==============================================================================
Omnibus:                     3230.499   Durbin-Watson:                   1.990
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             7578.212
Skew:                           1.116   Prob(JB):                         0.00
Kurtosis:                       5.479   Cond. No.                     1.53e+04
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.53e+04. This might indicate that there are
strong multicollinearity or other numerical problems.

======================================================================
Ecuaci√≥n del modelo:
======================================================================

y = 41276.4598¬∑median_income + 1966.9217¬∑housing_median_age + 9.1386¬∑total_rooms + -11.3958¬∑population + -17219.7617

======================================================================
Metricas:
======================================================================
MSE: 6602599478.6544
RMSE: 81256.3812
R¬≤: 0.5172
R¬≤ ajustado: 0.5171
```

### **Interpretaci√≥n:**

- **P-valores:** Statsmodels muestra que todas las variables (median_income, housing_median_age, total_rooms, population) tienen p-valores < 0.001 (estad√≠sticamente significativas). Esto confirma que todas contribuyen significativamente al modelo.
- **F-statistic = 4,377:** Valor muy alto con p-valor ‚âà 0 confirma que el modelo en conjunto es estad√≠sticamente significativo. Podemos afirmar con m√°s del 99.9% de confianza que el modelo es mejor que simplemente predecir la media.
- **R¬≤ vs R¬≤ ajustado:** La diferencia m√≠nima (0.5172 vs 0.5171 ‚âà 0.0001) indica que las 4 variables a√±adidas son todas √∫tiles y no estamos "inflando" artificialmente el R¬≤.
- **Durbin-Watson = 1.990:** Muy cercano a 2.0 (ideal), lo que indica que no hay autocorrelaci√≥n en los residuos. El supuesto de independencia se cumple.
- **Condition Number = 15,300:** Valor alto sugiere posible multicolinealidad moderada. Deber√≠as verificar con VIF para asegurar que las variables no est√°n demasiado correlacionadas.
- **Omnibus/Jarque-Bera (p = 0.000):** Los residuos no son perfectamente normales (Skew = 1.12 indica asimetr√≠a). Hay algunas casas muy caras que el modelo no captura bien. Con n = 16,346, las estimaciones siguen siendo robustas.
- **AIC = 415,500 / BIC = 415,500:** Valores que permiten comparar este modelo con otros. *Menor* **AIC/BIC** = `mejor modelo`.
- **Conclusi√≥n:** Statsmodels confirma que la regresi√≥n m√∫ltiple mejora significativamente sobre la simple, y proporciona evidencia estad√≠stica s√≥lida de que las relaciones encontradas son reales (no debidas al azar). Todas las variables son significativas y el modelo es robusto.

### **Comparaci√≥n: Regresi√≥n Simple vs M√∫ltiple**

|**M√©trica**        |**Regresi√≥n Simple**    |**Regresi√≥n M√∫ltiple**  |**Mejora**            
|----------------|--------------------|--------------|-----------------|
|**Variables**   |1 (median_income)   |4 (income, age, rooms, population)     |+3 variables            |
|**RMSE**        |  ~$85,000   |$81,256           |-$3,744 (4.4% mejor)              |
|**R¬≤**         | 0.4720 (47.2%)|0.5172 (51.7%) |+4.5% varianza explicada              |
|**Interpretaci√≥n** |Predictor moderado      |  Predictor moderadamente bueno| Mejora notable |            

**Conclusi√≥n clave:** Al incorporar **m√°s variables** relevantes, **mejoramos la capacidad predictiva del modelo**. Sin embargo, a√∫n queda un **48.3%** de variabilidad **sin explicar**, sugiriendo que factores como `la ubicaci√≥n exacta` (latitude, longitude), `caracter√≠sticas espec√≠ficas` de la vivienda, o `la proximidad al oc√©ano` (ocean_proximity) podr√≠an **mejorar** a√∫n m√°s el modelo.

##  TABLA DE COMPARACI√ìN FINAL

### Comparaci√≥n: Scikit-learn vs Statsmodels (Regresi√≥n Lineal Simple y M√∫ltiple)

| **Aspecto** | **Scikit-learn** | **Statsmodels** |
|--------------|------------------|-----------------|
| üéØ **Prop√≥sito principal** | Machine Learning y predicci√≥n | An√°lisis estad√≠stico e inferencia |
| üìö **Facilidad de uso** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Muy f√°cil | ‚≠ê‚≠ê‚≠ê Moderada (requiere `add_constant`) |
| üìä **Regresi√≥n Simple** |  `LinearRegression()` |  `OLS(y, X)` |
| üìà **Regresi√≥n M√∫ltiple** |  `LinearRegression()` (mismo m√©todo) |  `OLS(y, X)` (mismo m√©todo) |
| üîß **Intercepto** | Autom√°tico |  Manual (`sm.add_constant()`) |
| üìê **Sintaxis de entrenamiento** | `model.fit(X, y)` | `model.fit()`|
| üé≤ **Coeficientes** | `model.coef_`, `model.intercept_` | `results.params.iloc[0]`, `results.params.iloc[1:]` |
| üìä **M√©tricas b√°sicas** | R¬≤, MSE (con funciones separadas) |  R¬≤, R¬≤ ajustado (autom√°tico) |
| üìâ **P-valores** | ‚ùå No proporciona | ‚úÖ S√≠ (`results.pvalues`) |
| üìè **Intervalos de confianza** | ‚ùå No proporciona | ‚úÖ S√≠ (95% por defecto) |
| üîç **Resumen estad√≠stico** | ‚ùå No incluido | ‚úÖ `results.summary()` muy completo |
| üìä **Tests diagn√≥stico** | ‚ùå Debes calcular manualmente | ‚úÖ Autom√°ticos (Durbin-Watson, Omnibus, JB) |
| üî¢ **F-statistic** | ‚ùå No proporciona | ‚úÖ S√≠ |
| üìê **AIC/BIC** | ‚ùå No proporciona | ‚úÖ S√≠ |
| ‚ö° **Velocidad** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Muy r√°pida | ‚≠ê‚≠ê‚≠ê‚≠ê R√°pida |
| üíæ **Persistencia** | ‚úÖ F√°cil con `pickle` | ‚úÖ Posible con `pickle` |
| üîó **Integraci√≥n pipelines** | ‚úÖ Excelente | ‚ö†Ô∏è Limitada |
| üìñ **Documentaci√≥n** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excelente | ‚≠ê‚≠ê‚≠ê‚≠ê Muy buena |
| üë• **Comunidad** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Enorme | ‚≠ê‚≠ê‚≠ê‚≠ê Grande |
| üì¶ **Tama√±o instalaci√≥n** | ~50 MB | ~30 MB |
| üéì **Curva aprendizaje** | Baja | Media |
| ‚úÖ **Mejor para...** | Producci√≥n, ML, predicci√≥n | Investigaci√≥n, an√°lisis estad√≠stico, reportes |
| ‚ùå **NO recomendado para...** | An√°lisis estad√≠stico profundo | Producci√≥n con pipelines complejos |

---

### Resumen de Recomendaciones

| **Caso de Uso** | **Librer√≠a Recomendada** | **Raz√≥n** |
|------------------|--------------------------|------------|
| Proyecto de Machine Learning |  Scikit-learn | API simple, integraci√≥n con pipelines, r√°pida |
| An√°lisis estad√≠stico acad√©mico |  Statsmodels | P-valores, intervalos de confianza, tests completos |
| Producci√≥n / Aplicaciones |  Scikit-learn | M√°s f√°cil de integrar y desplegar |
| Reportes cient√≠ficos |  Statsmodels | Resumen estad√≠stico detallado |
| Necesitas p-valores |  Statsmodels | √önico que los proporciona |
| Solo necesitas predicciones |  Scikit-learn | M√°s eficiente para este prop√≥sito |
| Aprendizaje / Ense√±anza |  Scikit-learn | M√°s intuitiva para principiantes |

---

## EJERCICIOS PR√ÅCTICOS (ADAPTADOS AL DATASET DIABETES)
### **Ejercicio 1:** `Regresi√≥n Lineal Simple`

> **Objetivo:** Predecir la progresi√≥n de la diabetes bas√°ndose en una sola variable m√©dica.
**Datos:** Usa el dataset **Diabetes** incluido en scikit-learn.

**Tareas:**
 1. Carga el dataset Diabetes de scikit-learn
2. Selecciona una variable independiente (ej: bmi) y la variable dependiente (target)
3. Divide los datos en train (80%) y test (20%)
4. Entrena un modelo de regresi√≥n lineal simple con Scikit-learn
5. Calcula MSE y R¬≤ en el conjunto de test
6. **Crea un gr√°fico con:**
    - Puntos de datos reales
    - L√≠nea de regresi√≥n
    - Ecuaci√≥n del modelo en el t√≠tulo

**C√≥digo base:**
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Cargar dataset
data = load_diabetes()

# TODO: Completar el ejercicio

# Tu c√≥digo aqu√≠...
```

###  **Ejercicio 2:** `Regresi√≥n Lineal M√∫ltiple`

> **Objetivo:** Predecir la progresi√≥n de la diabetes usando m√∫ltiples variables m√©dicas.

**Variables:**
- **Independientes:** `bmi`, `bp`, `s5`
- **Dependiente:** `target`

**Tareas:**
1. Divide en train/test (80/20)
2. Entrena modelo de regresi√≥n m√∫ltiple
3. Calcula MSE y R¬≤ para train y test

4. **Crea dos gr√°ficos:**
    - Predicciones vs Valores Reales (train)
    - Predicciones vs Valores Reales (test)
5. Muestra la ecuaci√≥n completa del modelo

**C√≥digo base:**
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_diabetes
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Cargar dataset
data = load_diabetes()
df = pd.DataFrame(data.data, columns = data.feature_names)
df['target'] = data.target

# TODO: Completar el ejercicio

# Tu c√≥digo aqu√≠...
```
## Nota sobre el Dataset *Diabetes*:
El dataset contiene 10 variables m√©dicas estandarizadas (media = 0, std = 1):

| Variable | Descripci√≥n                                     |
| -------- | ----------------------------------------------- |
| `age`    | Edad                                            |
| `sex`    | Sexo                                            |
| `bmi`    | √çndice de masa corporal                         |
| `bp`     | Presi√≥n arterial promedio                       |
| `s1`     | `tc` = colesterol total s√©rico                  |
| `s2`     | `ldl` = lipoprote√≠nas de baja densidad          |
| `s3`     | `hdl` = lipoprote√≠nas de alta densidad          |
| `s4`     | `tch` = proporci√≥n colesterol total / HDL       |
| `s5`     | `ltg` = log de niveles de triglic√©ridos s√©ricos |
| `s6`     | `glu` = nivel de glucosa en sangre              |


La variable `target` mide la progresi√≥n de la enfermedad un a√±o despu√©s del inicio
## 5. Recursos Online

### 5.1. Documentaci√≥n Oficial

1. **Scikit-learn:**
   - https://scikit-learn.org/stable/modules/linear_model.html
   - Tutorial completo de regresi√≥n lineal

2. **Statsmodels:**
   - https://www.statsmodels.org/stable/regression.html
   - Ejemplos de an√°lisis estad√≠stico

3. **TensorFlow:**
   - https://www.tensorflow.org/tutorials/keras/regression
   - Tutorial de regresi√≥n con Keras

4. **PyTorch:**
   - https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html
   - Fundamentos de optimizaci√≥n

### 5.2. Tutoriales y Blogs

1. **Real Python - Linear Regression**
   - https://realpython.com/linear-regression-in-python/
   - Tutorial paso a paso muy claro

2. **Machine Learning Mastery**
   - https://machinelearningmastery.com/linear-regression-for-machine-learning/
   - Gu√≠a pr√°ctica con ejemplos

3. **Towards Data Science**
   - https://towardsdatascience.com/tagged/linear-regression
   - Art√≠culos variados sobre regresi√≥n

4. **StatQuest (YouTube)**
   - https://www.youtube.com/c/joshstarmer
   - Videos explicativos excelentes

5. **3Blue1Brown (YouTube)**
   - https://www.youtube.com/c/3blue1brown
   - Intuici√≥n geom√©trica de √°lgebra lineal

### 5.4. Datasets de Pr√°ctica

1. **Kaggle:**
   - https://www.kaggle.com/datasets
   - Miles de datasets reales

2. **UCI Machine Learning Repository:**
   - https://archive.ics.uci.edu/ml/
   - Datasets cl√°sicos

3. **Scikit-learn Datasets:**
   - Incluidos en la librer√≠a
   - `sklearn.datasets`

---

### Herramientas √ötiles

1. **Jupyter Notebook**
   - Ideal para experimentaci√≥n
   - `pip install jupyter`

2. **Google Colab**
   - https://colab.research.google.com/
   - Jupyter en la nube gratis

3. **Anaconda**
   - https://www.anaconda.com/
   - Distribuci√≥n Python completa